# 1. 주성분분석 개요
## 1. PCA(Principal Components Analysios)의 개념
- 여러 개의 변수 중 서로 상관성이 높은 변수들의 선형 결합으로 새로운 변수(주성분)를 만들어 기존 변수를 요약 및 축소하는 분석 방법이다.

## 2. PCA의 목적
- 변수를 축소하여 모형의 설명력을 높임
- 다중공선성 문제를 해결
- 군집분석 시 모형의 성능을 높일 수 있음
- IoT 센서 데이터를 PCA 후 스마트팩토리에 활용
- PCA 시 선형변환이 필요함

## 3. PCA 방법
- 주어진 데이터를 하나의 변수로 요약한다는 것은 하나의 관점에서 바라보는 것을 의미하는데, 그에 따른 데이터 손실이 발생할 수 밖에 없다.
- 데이터를 바라볼 수 있는 관점은 다양하지만 그 중 손실이 가장 작은 축을 찾는 것, 즉 자료의 분산이 가장 큰 축을 찾아 새로운 변수로 만드는 방법이다.
- 데이터를 가장 잘 표현하는 직교상의 데이터 벡터들을 찾아서 데이터 압축한다. 속성들을 선택하고 조합하여 다른 작은 집합을 생성한다.
- 계산이 간단하며 데이터 부족이나 일률적 데이터 혹은 정렬되지 않은 속성을 가진 데이터도 처리할 수 있다는 장점이 있다.
- 고윳값이란 고유 벡터의 크기를 의미하며 해당 값이 클수록 높은 설명력을 갖는다. 이때, 고윳값들의 평균을 구한 뒤 고윳값이 평균보다 작은 값을 갖는 주성분을 제거하는 것을 편균 고윳값 방법이라 한다.
- 주성분분석을 이해하기 위해, 책상 위에 물병 3개가 놓여 있는 상황을 예로 들어보자. X축과 Y축 관점에서 보면 물병의 위치를 정확히 알 수 있다. 그러나 두 개의 관점을 하나의 관점으로 압축하기 위해 Y축, X축, 제1주성분 관점에서 바라볼 때를 비교해보면 분산이 가장 큰 제1주성분이 정보손실이 가장 적다는 것을 알 수 있다.

# 2. PCA 사례 실습
## 1. 주성분분석#1 - 수행 및 해석

```
> # PCA 
> cal <- c(52, 160, 89, 57, 34, 32, 30, 69)
> car <- c(112.4, 8.5, 22.8, 14.5, 8.2, 7.7, 7.6, 18.1)
> fat <- c(0.2, 14.7, 1.3, 0.7, 0.2, 0.3, 0.2, 0.2)
> pro <- c(0.3, 2.0, 1.1, 0.3, 0.8, 0.7, 0.6, 0.7)
> fib <- c(2.4, 6.7, 2.6, 2.4, 0.9, 2.0, 0.4, 0.9)
> sug <- c(10.4, 0.7, 12.2, 9.9, 7.9, 4.7, 6.2, 15.5)
> fruits <- data.frame(cal, car, fat, pro, fib, sug)
> result <- prcomp(fruits, center = T, scale. = T)
> # center와 scale 값을 T로 표준화하여 변수 간 단위 차이를 제거거
> result
Standard deviations (1, .., p=6):
[1] 2.01314374 1.05853762 0.81088150 0.35447821 0.19925394 0.06216644

Rotation (n x k) = (6 x 6):
           PC1         PC2         PC3         PC4          PC5        PC6
cal  0.4497789  0.25668231  0.39132362 -0.13770784  0.172277348 -0.7280357
car -0.1343382  0.86196398 -0.34816436  0.34088787  0.031790681 -0.0231893
fat  0.4899044  0.06599968 -0.03435189 -0.04290641  0.718838104  0.4856835
pro  0.4638167 -0.11821949  0.23112276  0.75978379 -0.345838088  0.1435447
fib  0.4608776  0.27563218 -0.08775242 -0.53083464 -0.577034897  0.2986026
sug -0.3348309  0.31117101  0.81446248 -0.06272348 -0.002897031  0.3518084
> summary(result)
Importance of components:
                          PC1    PC2    PC3     PC4     PC5     PC6
Standard deviation     2.0131 1.0585 0.8109 0.35448 0.19925 0.06217
Proportion of Variance 0.6755 0.1867 0.1096 0.02094 0.00662 0.00064
Cumulative Proportion  0.6755 0.8622 0.9718 0.99274 0.99936 1.00000
```

- 첫 번째 주성분
    - PC1 = (0.4497789 * cal) + (-0.1343382 * car) + (0.4899044 * fat) + (0.4638167 * pro) + (0.4608776 * fib) + (-0.3348309 * sug)
    - 첫 번째 주성분은 계수가 큰 cal, pro, fat, fib에 영향을 많이 받는다.

- 두 번째 주성분
    - PC2 = (0.25668231 * cal) + (0.86196398 * car) + (0.06599968 * fat) + (-0.11821949 * pro) + (0.27563218 * fib) + (0.31117101 * sug)
    - 두 번째 주성분은 게수가 큰 car에 많은 영향을 받는다.
---
- PC1은 전체 데이터의 67.55%를 설명한다.
- PC1 하나의 누적 설명력은 67.55%이다.
- PC2는 전체 데이터의 18.67%를 설명한다.
- PC1, PC2 두 성분의 누적 설명력은 86.22%이다.
- 주성분의 개수는 전체 데이터의 70% 이상을 설명할 수 있도록 선택한다.
- 따라서 2개의 주성분으로 전체 데이터의 86.22%를 설명할 수 있기 때문에 기존 6개의 변수로 설명하던 8개의 과일을 2개의 변수로 설명할 수 있다.

## 2. 주성분분석#2 - scree plot
- scree plot은 x축을 성분의 개수, y축을 고윳값(eigenvalue)으로 하는 그래프로 주성분의 개수를 선택하는데 도움을 준다. 일반적으로 고윳값이 1 근처의 값을 갖는 PCA의 수를 결정할 수 있다. 그러나 또 다른 방법으로는 그래프가 수평을 이루기 전 단계를 주성분의 수로 선택할 수 있다.

```
screeplot(result, type = 'lines')
```

## 3. 주성분분석#3 - biplot
- biplot은 첫 번째 주성분과 두 번째 주성분을 축으로 하는 그래프다.
- biplot 그래프는 다차원 척도법과 같이 주성분분석의 결과로 데이터가 어떻게 퍼져 있는지 시각화할 수 있다.

## 차원의 저주
일반적으로 고차원의 데이터일수록 데이터에 대한 설명력이 좋아질 것으로 생각한다. 그러나 그것은 데이터가 충분히 많은 경우에만 해당하며, 그렇지 않은 경우에는 데이터 사이의 밀도가 감소하게 된다. 따라서 불충분한 데이터 수에 어울리지 않는 고차원의 데이터는 설명에 있어서 근거가 부족해지며 구축된 모델의 성능에 부정적인 결과를 가져온다.

