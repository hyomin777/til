# 1. 회귀분석의 개념
## 1. 회귀분석
- 회귀분석이란 하나 이상의 독립변수($x_1, x_2, x_3, ...$)들이 종속변수(y)에 얼마나 영향을 미치는지 추정하는 통계기법이다.
- 독립변수와 종속변수 간에 인과관계가 있다는 말은 독립변수가 원인이 되어 종속변수에 영향을 미친다는 의미다. 그런 의미에서 독립변수를 원인변수(혹은 설명 변수) 종속변수를 결과변수(혹은 반응 변수)라고도 한다.
- 독립변수가 하나이면 단순선형회귀분석, 2개 이상이면 다중선형회귀분석으로 분석할 수 있다.
- 회귀분석은 기본적으로 변수가 연속형 변수일 때 사용하며, 범주형 변수일 경우 파생변수로 변환하여 사용한다. 만약에 종속변수가 범주형일 경우 로지스틱 회귀분석을 사용한다.
- 변수들이 일정한 경향성을 띤다는 의미는 그 변수들이 일정한 인과관계를 갖고 있다고 추측할 수 있다. 따라서 산점도를 봤을 때 일정한 추세선이 나타난다면, 경향성을 가지거나 혹은 변수들 간에 인과관계가 존재한다고 미루어 생각할 수 있다.
- 분석을 하기 전 미리 EDA(Exploratory Data Analysis)를 통해 산점도를 그려보면 변수들 간에 어떤 의미있는 관계가 있는지 미리 짐작할 수 있다.

## 2. 회귀분석의 종류
- 단순회귀
  - $Y = \beta_0 + \beta_1 X+\varepsilon$
  - 1개의 독립변수와 종속변수가 직선(선형) 관계
- 다중회귀
  - $Y = \beta_0+\beta_1 X_1+\beta_2 X_2 + ... + \beta_n X_n + \varepsilon$
  - n개의 독립변수와 종속변수가 선형 관계
- 다항회귀
  - $Y=\beta_0+\beta_1 X^1+\beta_2 X^2+...+\beta_k X^k+\varepsilon$
  - 1개의 독립변수와 종속변수가 2차 함수 이상의 관계
- 다중다항회귀
  - $Y=\beta_0+\beta_1 X^a_1+\beta_2 X^b_2+...+\beta_n X^m_n+\varepsilon$
  - n개의 독립변수와 종속변수가 2차 함수 이상의 관계

### 회귀(Regression)의 의미
- '회귀'라는 말에는 '돌아온다'는 의미가 담겨있다. 'Regression'은 옛날 상태로 돌아간다는 의미를 담고 있다. 그렇다면 인과관계를 밝히는 분석에 왜 '회귀'라는 용어를 사용했을까?
- '회귀'의 원래 의미는 옛날 상태로 돌아가는 것을 의미한다. 영국의 유전학자 프랜시스 골턴은 부모의 키와 아이들의 키 사이의 연관 관계를 연구하면서 부모와 자녀의 키 사이에는 선형적인 관계가 있고 키가 커지거나 작아지는 것보다는 전체 키 평균으로 돌아가려는 경향이 있다는 가설을 세웠으며 이를 분석하는 방법을 '회귀분석'이라고 하였다.
- 이러한 경험적 연구 이후, 칼 피어슨은 아버지와 아들의 키를 조사한 결과를 바탕으로 함수 관계를 도출하여 회귀분석 이론을 수학적으로 정립하였다.

### 인과관계가 있어야 '회귀'
- X축의 값이 증가하면서 일정하게 Y축의 값도 변하는 데이터를 점으로 표현하고 그 추세선을 그렸을 때 수학자 골턴은 마치 데이터들이 추세선을 따라 회귀한다고 보았고 그 이름을 회귀분석이라 지었다. 하지만 최근에는 데이터를 주인공으로 보고 데이터를 따라 그려보니 추세선이 진행(Progression)한다고 보는 학자들도 있다.
- 하지만 이러한 선형 그래프만으로는 X축과 Y축의 두 변수가 서로 인과관계가 있다고 단정하기는 어렵다. 상관분석에서 그렇듯 상관성은 있지만 인과관계가 반드시 존재한다고 볼 수는 없기 때문이다.
- 왜냐하면 인과관계가 존재하려면 몇 가지 전제조건이 필요하다. 선형성, 독립성, 등분산성, 정규성, 비상관성 등이 그것이다.
- 그렇다면 이 몇 가지 전제조건이 있다면 두 변수는 인과관계가 있다고 볼 수 있을까? 정답은 '아니오'다. 말 그대로 전제조건이며 인과관계가 있다고 밝히기 위해서는 회귀분석을 사용하여 회귀결정계수를 도출하고 그 회귀결정계수를 판별하여 비로소 인과관계를 말할 수 있다.

### 회귀분석과 예측
- 예측을 위한 분석에 왜 회귀분석의 알고리즘이 사용될까?
- 회귀분석은 원인과 결과의 인과관계에 기반한 분석이다. 이 말의 의미를 풀어보면 원인과 결과 사이에 함수를 만들수도 있다는 의미다. 그렇다면 그 회귀함수를 이용하여 예측도 가능하다는 말과 일맥상통한다. 그래서 회귀함수는 데이터를 통해 예측 혹은 분류하는 데이터 마이닝에 알고리즘으로 사용되기도 한다.
- 설명력이 높은 회귀함수가 만들어졌다고 가정해보자. 함수의 설명력이 높다는 말은 인과관계가 '크다 혹은 강하다'라는 말과 같으며, 그 함수에 미지의 값을 입력했을 때 그 결괏값의 예측력이 매우 높다는 말과 같다. 그래서 설명력이 높은 회귀함수(방정시그 모델)를 만들면 정확도가 높은 예측이 가능하여 많은 데이터 분석에 활용된다.
- 회귀결정계수가 1과 가깝다면 아주 강한 인과관계가 있다고 판단하고, 반대로 0과 가깝다면 약한 인과관계가 있다고 판단한다. 회귀결정계수와 회귀계수는 다르다.
- 회귀함수에서 회귀계수가 크다는 말은 곧 X축 값(독립변수)의 변화에 Y축 값(종속변수)이 매우 민감하게 반응한다는 이야기이며, 이는 인과관계가 강하다는 말과 같다. 회귀계수는 회귀함수의 기울기를 의미하며, 독립변수가 변할 때 종속변수에 미치는 영향의 정도를 의미한다.

# 2. 회귀분석의 가정
## 1. 선형성
- 독립변수와 종속변수가 선형적이여야 한다.
- 예외적으로 2차함수 회귀선을 갖는 다항회귀분석의 경우에는 선형성을 갖지 않아도 된다.
- 산점도를 통해 분석하기 전에 변수 사이의 관계를 짐작할 수 있어 회귀분석 하기 전 상관분석은 거의 필수적으로 함께 따라온다.

## 2. 독립성 
- 단순회귀분석에서는 잔차와 독립변수의 값이 서로 독립이어야 한다.
- 독립변수가 여러 개인 다중회귀분석의 경우에는 독립변수들 간에 상관성이 없이 독립이어야 한다.
- 만약 독립변수들 간에 상관성이 존재하는 경우, 이를 다중공선성이라 하며, 이를 제거하고 회귀분석을 수행해야 한다.

## 3. 등분산성
- 등분산성이란, 분산이 같다는 의미이며 다른 말로 잔차들이 고르게 분포하고 있다는 의미다.
- 잔차의 중심에서 분산이 같아야 한다는 의미다. 등분산성을 만족하지 못하면 회귀선은 어떤 추세를 띄지 못하고 덩어리(뭉친) 모양을 하게 된다.

## 4. 정규성
- 잔차항이 정규분포 형태를 띠는 것을 정규성을 만족한다고 한다.
- Q-Q Plot에서 잔차가 오른쪽으로 상승하는 형태를 띠면 정규성을 만족한다고 판단한다.
- 정규성을 검증하기 위한 방법으로는 히스토그램, Q-Q Plot을 활용하여 시작적으로 확인할 수 있으며 샤피로 검정, 앤더슨-달링 검정, 하르케-베라 검정 등이 있다.

### 오차와 잔차
- 모집단의 데이터를 활용하여 회귀식을 구한 경우 예측 값과 실제 값의 차이를 오차라 한다.
- 그러나 모집단을 특정할 수 없는 경우 모집단의 일부인 표본집단으로 회귀식을 추정하게 되는데, 이때 표본집단에 의해 추정된 회귀식의 예측 값과 실제 값의 차이를 잔차라 한다.
- 즉, 모집단에서 오차, 표본집단에서 잔차라고 부른다.

### 잔차도
- 예측 값과 실제 값의 차이를 나타낸 산점도를 잔차도라 한다.
- 예측 값과 실제 값의 차이가 없는 경우에는 잔차가 0이다.
- 잔차도를 활용하여 회귀식의 선형성, 등분산성의 가정성의 위배 여부 및 그에 대한 해결책을 찾을 수 있다.